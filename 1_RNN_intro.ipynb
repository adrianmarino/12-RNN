{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys \n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN (Recurrent Neural Networks)\n",
    "Articulos recomendado: \n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- http://blog.echen.me/2017/05/30/exploring-lstms/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Que tienen de nuevo respecto a MLP y CNN?\n",
    "- MLP y CNN solo aceptan un vector de entrada de tamaño fijo y devuelve un vector de salida de tamaño fijo\n",
    "- RNN trabjan con secuencias tanto a la entrada como a la salida\n",
    "- No tienen por que estrictamente tener una secuencia ni a la entrada ni a la salidad. De hecho hasta podrían no tener nada a la \"entrada\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn_types.jpeg](rnn_types.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aclaraciones: \n",
    "- Cada cuadrado NO es una neurona si no una capa que puede contener N neuronas\n",
    "- Cada flecha representa la interconexión entre dos capas. Los pesos forman una matriz de la N1xN2 donde N1 y N2 son la cantidad de neuronas en cada capa respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipos\n",
    "- **One to One**: CNN, MLP\n",
    "- **One to many**: [Image captioning](https://www.youtube.com/watch?v=xKt21ucdBY0)\n",
    "- **Many to one**: Sentiment Analisys, Detectar voz de hombre vs voz de mujer\n",
    "- **Many to Many** [(sequence to sequence)](https://youtu.be/dkHdEAJnV_w): [Traducción](https://github.com/jganzabal/aind2-nlp-capstone/blob/master/machine_translation.ipynb) Dimensión de entrada differente a la de salida.\n",
    "- **Many to Many Sincronizado**: Etiquetado de tramas de video, POS (Part of Speech) cada palabra se clasifica en verbo, articulo, etc, speech2text, text2speech, NER (Named Entity Recognition). Dimensión de entrada igual a dimensión de salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de lenguaje Generativos:\n",
    "- Predecir la proxima palabra en funcion de las anteriores\n",
    "- Predecir el proximo caracter en función de los anteriores\n",
    "\n",
    "**Resultado**: Probabilidad dada la secuencia de caracteres o de palabras\n",
    "\n",
    "**Aplicaciones de los modelos de lenguaje** (Mas allá de la posibilidad de generar texto):\n",
    "- OCR\n",
    "- Speach2Text\n",
    "- Detección de autores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"If training vanilla neural nets is optimization over functions, training recurrent nets is optimization over programs.\"\n",
    "- Es una secuencia de ejecución mas que una clasificación\n",
    "- Las RNN son Turing completo en principio [Turing Complete](https://en.wikipedia.org/wiki/Turing_completeness), [RNN Turing complete](http://binds.cs.umass.edu/papers/1995_Siegelmann_Science.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detalles de la arquitectura:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unidad de Elman o RNN unit\n",
    "\n",
    "**Nota importante**: Cada unidad no es una neurona sino que una capa\n",
    "\n",
    "![RNN_vs_FNN.png](RNN_vs_FNN.png)\n",
    "\n",
    "¿Cual es el tamaño de $W_h$?\n",
    "\n",
    "Todo se conecta con todo -> Si hay M hidden units tenemos $M^2$ $W_h$s "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En ecuaciones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFN\n",
    "\n",
    "$h_t = f(W_x^T X_t + b_h)$\n",
    "\n",
    "$y_t = softmax(W_o^T h_t + b_o)$\n",
    "\n",
    "$f$ usualmente RELU, sigmoidea, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h_t = f(W_h^T h_{t-1} + W_x^T X_t + b_h)$\n",
    "\n",
    "$y_t = softmax(W_o^T h_t + b_o)$\n",
    "\n",
    "$f$ es tanh usualmente pero puede ser RELU, sigmoid, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo con dos neuronas en la capa oculta\n",
    "\n",
    "La idea es comprender paso a paso los calculos en cada paso de la red hasta hallar el valor de la salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![unfold_RNN.png](unfold_RNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero calculamos la salida(ht) de la funcion de activacion 'f' que en este caso va a ser una 'tanh':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M: 2\n",
      "h(t=0):\n",
      "[[ 1]\n",
      " [-2]]\n",
      "Wh:\n",
      "[[ 0 -1]\n",
      " [ 2  1]]\n",
      "X(t=1):\n",
      "[[ 1]\n",
      " [-1]\n",
      " [ 2]]\n",
      "Wx:\n",
      "[[-1  1]\n",
      " [ 1  0]\n",
      " [ 2  1]]\n",
      "bh:\n",
      "[[-1]\n",
      " [ 1]]\n",
      "\n",
      "Cantidad de parametros de recurrente(que la red tiene que aprender/ajustar): 12\n"
     ]
    }
   ],
   "source": [
    "print_matrix = lambda name, matrix: print(f'{name}:\\n{matrix.view()}')\n",
    "\n",
    "# M: Cantidad de neuronas en la capa oculta.\n",
    "M = 2\n",
    "print('M:', M)\n",
    "\n",
    "# h0: Valores iniciales de la capa oculta, vector columna de Mx1.\n",
    "h0 = np.array([1, -2]).reshape(2, 1)\n",
    "print_matrix('h(t=0)', h0)\n",
    "\n",
    "# Wh: Matriz de pesos de capa oculta MxM.\n",
    "Wh = np.array([[0, -1], [2, 1]])\n",
    "print_matrix('Wh', Wh)\n",
    "\n",
    "# Dimension de entrada 3 en este ejemplo.\n",
    "Xt = np.array([1, -1, 2]).reshape(3,1) # Vector columna\n",
    "print_matrix('X(t=1)', Xt)\n",
    "\n",
    "# Dimension de Wx?\n",
    "# Re: Dimensión 3x2 para que Wx.T sea de 2x3\n",
    "Wx = np.array([[-1, 1], [1, 0], [2, 1]])\n",
    "print_matrix('Wx', Wx)\n",
    "\n",
    "\n",
    "# Dimensión Mx1\n",
    "bh = np.array([-1, 1]).reshape(2,1)\n",
    "print_matrix('bh', bh)\n",
    "\n",
    "\n",
    "total_params_r_layer = Wh.shape[0] * Wh.shape[1] + Wx.shape[0] * Wx.shape[1] + bh.shape[0]\n",
    "\n",
    "print('\\nCantidad de parametros de recurrente(que la red tiene que aprender/ajustar):', total_params_r_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  2],\n",
       "       [-1,  1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wh.T # T: Roto en direccion agujas y luego mirror."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4],\n",
       "       [-3]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wh.T.dot(h0) # Producto escalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [3]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " Wx.T.dot(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Argumneto de h(t):\n",
      "[[-3]\n",
      " [ 1]]\n",
      "h(t=1):\n",
      "[[-0.99505475]\n",
      " [ 0.76159416]]\n"
     ]
    }
   ],
   "source": [
    "h_arg = Wh.T.dot(h0) + Wx.T.dot(Xt) + bh\n",
    "h1 = np.tanh(h_arg) # Ver grafico tanh!\n",
    "\n",
    "print(f'Argumneto de h(t):\\n{h_arg}')\n",
    "print(f'h(t=1):\\n{h1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1200px-Hyperbolic_Tangent.svg.png](1200px-Hyperbolic_Tangent.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcula de la salida de la softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wo:\n",
      "[[0.5 0.4 0.3]\n",
      " [0.5 0.1 0.5]]\n",
      "ao:\n",
      "[[ 0. ]\n",
      " [ 0.3]\n",
      " [-0.5]]\n",
      "Salida:\n",
      " [[0.33825043]\n",
      " [0.45659032]\n",
      " [0.20515925]]\n",
      "\n",
      "Cantidad de parametros de capa densa de salida: 9\n"
     ]
    }
   ],
   "source": [
    "# Salida de 3 neuronas en la capa oculta:\n",
    "\n",
    "Wo = np.array([[0.5, 0.4],[0.3, 0.5],[0.1, 0.5]]).reshape(2,3)\n",
    "bo = np.array([0.5, 0.1, 0.2]).reshape(3,1)\n",
    "ao = Wo.T.dot(h0) + bo\n",
    "\n",
    "print_matrix('Wo', Wo)\n",
    "print_matrix('ao', ao)\n",
    "\n",
    "softmax = lambda x: np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "print('Salida:\\n', softmax(ao))\n",
    "\n",
    "total_params_out_layer = Wo.shape[0] * Wo.shape[1] + bo.shape[0]\n",
    "\n",
    "print('\\nCantidad de parametros de capa densa de salida:', total_params_out_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Con keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import SimpleRNN, Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_neurons = 2 # M\n",
    "time_steps = 1000 # T\n",
    "n_features = 3 # D\n",
    "input_shape = (time_steps, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(rnn_neurons, input_shape = input_shape))\n",
    "model.add(Dense(3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desplegando (Unfolding) la RNN\n",
    "#### BPTT (Back-Propagation Through Time)\n",
    "- Secuencia de longitud 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![unfold_RNN.png](unfold_RNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observaciones:\n",
    "- Es como una FNN con 5 hidden layers pero con pesos compartidos (Shared weights): $W_h, W_o, W_x$\n",
    "- Donde habiamos visto pesos compartidos?\n",
    "- Se puede pensar como si h fuera la entrada y X es una señal de control en cada paso\n",
    "\n",
    "Preguntas:\n",
    "- Son todas las Y importantes? En que casos?\n",
    "- Que diferencia hay entre estado interno (internal state) de la RNN y los pesos?\n",
    "- Cada cuanto se resetea el estado interno? Y los pesos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detalles de la entrada $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrada en Vanilla Networks:\n",
    "\n",
    "NxD, donde N es la cantidad de muestras y D es la cantidad de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo N = 3, D = 2\n",
    "Ex_1 = np.array([\n",
    "    [0.5, 0.3], \n",
    "    [0.2, 0.1], \n",
    "    [0.7, 0.3]\n",
    "])\n",
    "print(Ex_1)\n",
    "print(Ex_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrada en RNNs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secuencias de longitud fija\n",
    "\n",
    "NxTxD, donde T es la longitud de la secuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo N = 4, T = 3, D=2\n",
    "Ex_2 = np.array([\n",
    "    [\n",
    "        [0.5, 0.3], \n",
    "        [0.2, 0.1], \n",
    "        [0.7, 0.3]\n",
    "    ], \n",
    "    [\n",
    "        [0.54, 0.1], \n",
    "        [0.23, 0.3], \n",
    "        [0.9, 0.1]\n",
    "    ], \n",
    "    [\n",
    "        [0.5, 0.3], \n",
    "        [0.2, 0.1], \n",
    "        [0.7, 0.3]\n",
    "    ], \n",
    "    [\n",
    "        [0.54, 0.1], \n",
    "        [0.23, 0.3], \n",
    "        [0.9, 0.1]\n",
    "    ]\n",
    "])\n",
    "print(Ex_2)\n",
    "print(Ex_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secuencias de longitud variable (1 <= T <= 3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ex_3 = np.array([\n",
    "    [\n",
    "        [0.5, 0.3], \n",
    "        [0.7, 0.3]\n",
    "    ], \n",
    "    [\n",
    "        [0.54, 0.1], \n",
    "        [0.23, 0.3], \n",
    "        [0.9, 0.1]\n",
    "    ], \n",
    "    [\n",
    "        [0.7, 0.3]\n",
    "    ], \n",
    "    [\n",
    "        [0.54, 0.1], \n",
    "        [0.23, 0.3], \n",
    "        [0.9, 0.1]\n",
    "    ]\n",
    "])\n",
    "print(Ex_3)\n",
    "print(f'Samples: {Ex_3.shape}')\n",
    "print(np.array(Ex_3[0]).shape)\n",
    "print(np.array(Ex_3[1]).shape)\n",
    "print(np.array(Ex_3[2]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En tensorflow es posible, en keras se suele hacer padding\n",
    "\n",
    "¿Que pasa cuando tenemos longitudes distintas? Analizar el unfolding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desventajas de padding\n",
    "- Podría haber una secuencia de longitud mayor en el test set. Que hacemos con eso?\n",
    "- Es probable que los casos de secuencia largas sean poco probables por lo que realizaremos multiplicaciones de matrices innecesarias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos de preparación de datos para una RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción del valor de la acción de Apple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predecir el valor de las acciones de una compañia.\n",
    "- Se utilizarán las 5 observaciones anteriores \n",
    "- Tenemos los datos de 138 días\n",
    "- ¿Cuanto vale N, T, D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "apple_stock = np.loadtxt('apple/normalized_apple_prices.csv')\n",
    "plt.plot(apple_stock)\n",
    "plt.show()\n",
    "print(f'Cantidad de samples: {len(apple_stock)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def window_transform_series(series, window_size):\n",
    "    # containers for input/output pairs\n",
    "    X, y = [], []\n",
    "\n",
    "    # Lenght of series\n",
    "    N = series.shape[0]\n",
    "    \n",
    "    # Generate inputs and outputs\n",
    "    for i in range(N - window_size):\n",
    "        X.append(series[i:i+window_size])\n",
    "        y.append(series[i+window_size])\n",
    "        if i <=3:\n",
    "            print(f'X({i}:{i+window_size}): {series[i:i+window_size]}, y({i+window_size}): {series[i+window_size]}')\n",
    "        \n",
    "\n",
    "    # reshape each \n",
    "    X = np.asarray(X)\n",
    "    X.shape = (X.shape[0:2])\n",
    "    \n",
    "    y = np.asarray(y)\n",
    "    y.shape = (len(y),1)\n",
    "    return X,y\n",
    "\n",
    "X_, y = window_transform_series(apple_stock, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_.shape)\n",
    "print(X_[0:3,:])\n",
    "print(y.shape)\n",
    "print(y[0:4,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por que no se toman lo 5 ultimos valores? Por que no hay un valor y correspondiente!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parece un detalle pero las capas recurrentes en Keras exigen el siguente formato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X_.reshape(X_.shape[0], X_.shape[1], 1)\n",
    "\n",
    "print(f'\\n\\n\\n===========>> (N, T, D): {X.shape} <<===========\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0:2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(apple_stock[:30])\n",
    "i1 = 0\n",
    "plt.plot(range(i1,i1+5),X_[i1])\n",
    "plt.scatter(i1+5, y[i1], marker='x', color='k')\n",
    "i1 = 10\n",
    "plt.plot(range(i1,i1+5),X_[i1])\n",
    "plt.scatter(i1+5, y[i1], marker='x', color='k')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos de lenguaje\n",
    "Predecir el siguiente caracter en función de los caracteres anteriores:\n",
    "- Utilizar one-hot encoding para los caracteres\n",
    "- Tamaño de ventana de 100\n",
    "- Cantidad total de caracteres de la obra: 67561\n",
    "- Cantidad de caracteres diferentes: 71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('federico-garcia-lorca/bodas_de_sangre.txt').read()\n",
    "number_of_chars = len(text)\n",
    "all_chars = sorted(set(text))\n",
    "print('-----------------------------------------------------------------------------------------------------')\n",
    "print('-----------------------------------------------------------------------------------------------------')\n",
    "print('Cantidad de caracteres: ' + str(number_of_chars))\n",
    "print('Cantidad de caracteres unicos: ' + str(len(all_chars)))\n",
    "print(all_chars)\n",
    "print('-----------------------------------------------------------------------------------------------------')\n",
    "print('-----------------------------------------------------------------------------------------------------')\n",
    "print()\n",
    "print(text[:997])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_transform_text(text, window_size, step_size):\n",
    "    # Containers for input/output pairs\n",
    "    inputs, outputs = [], []\n",
    "    \n",
    "    # This is the number of iterations taking \n",
    "    # into acount the step_size and the window_size\n",
    "    N = int((len(text) - window_size) / step_size)\n",
    "\n",
    "    # Get inputs and outputs\n",
    "    for i in range(0, N, step_size):\n",
    "        inputs.append(text[i:i+window_size])\n",
    "        outputs.append(text[i+window_size])\n",
    "        \n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "step_size = 1\n",
    "\n",
    "inputs, outputs = window_transform_text(text, window_size, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs[0])\n",
    "print()\n",
    "print('-----------------------------------------------------------------------------------------------------')\n",
    "print('Salida:',outputs[0])\n",
    "print('-----------------------------------------------------------------------------------------------------')\n",
    "print()\n",
    "print(inputs[1])\n",
    "print('-----------------------------------------------------------------------------------------------------')\n",
    "print('Salida:',outputs[1])\n",
    "print('-----------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Como ingresamos a la red con caracteres?\n",
    "### Categorical Data (One-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_io_pairs(text,chars, window_size,step_size):\n",
    "    num_chars = len(chars)\n",
    "    chars_to_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "    # cut up text into character input/output pairs\n",
    "    inputs, outputs = window_transform_text(text,window_size,step_size)\n",
    "    \n",
    "    # create empty vessels for one-hot encoded input/output\n",
    "    X = np.zeros((len(inputs), window_size, num_chars), dtype=np.bool)\n",
    "    y = np.zeros((len(inputs), num_chars), dtype=np.bool)\n",
    "    \n",
    "    # loop over inputs/outputs and tranform and store in X/y\n",
    "    for i, sentence in enumerate(inputs):\n",
    "        for t, char in enumerate(sentence):\n",
    "            if char not in chars_to_indices:\n",
    "                char = ' '\n",
    "            X[i, t, chars_to_indices[char]] = 1\n",
    "        out_char = outputs[i]\n",
    "        if out_char not in chars_to_indices:\n",
    "            out_char = ' '\n",
    "        y[i, chars_to_indices[out_char]] = 1\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = encode_io_pairs(text, all_chars, window_size, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Caracteres codificado')\n",
    "print(X[0,0:5].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('(N,T,D):')\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN en Keras\n",
    "Definamos una capa RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN, Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_neurons = 2\n",
    "time_steps = 1000# T\n",
    "n_features = 71 # D\n",
    "input_shape = (time_steps, n_features)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(rnn_neurons, input_shape = input_shape))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observaciones:\n",
    "- Crece la cantidad de parametros con la cantidad de time_steps? \n",
    "Re: No.\n",
    "- Como puedo utilizar un MLP para que tenga en cuenta los time_steps? Que desventajas tengo? \n",
    "Re: Cada entrada es un timestep. Desventaja supongo que la memoria es elnumero de entradas y no se concerva estado entre ejecuciones.\n",
    "- rnn_neurons = 1, n_features = 1, por que es 3? Cuales son en el diagrama?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_number_of_parameters = lambda rnn_neurons, n_features: rnn_neurons * n_features + rnn_neurons**2 + rnn_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_number_of_parameters(1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleRNN + Dense para predecir caracteres\n",
    "Armar modelo con los siguientes datos:\n",
    "N,T,D = (67461, 100, 71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (67461, 100, 71)\n",
    "rnn_neurons = 200 # Hyperparametro para jugar\n",
    "time_steps = 100 # T\n",
    "n_features =  71 # D\n",
    "\n",
    "input_shape = (time_steps, n_features)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(rnn_neurons, input_shape = input_shape, return_sequences=True))\n",
    "model.add(Dense(n_features, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observaciones:\n",
    "- La cantidad de neuronas de la RNN queda para jugar (Overfitting, underfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleRNN + Dense para stock prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 40\n",
    "rnn_cells = 10\n",
    "\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(SimpleRNN(rnn_cells, input_shape = (window_size,1)))\n",
    "model_rnn.add(Dense(1))\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observaciones:\n",
    "- Capa densa. Que función de activación estamos usando? Lineal f(x)= x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un error común"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn_error = Sequential()\n",
    "model_rnn_error.add(SimpleRNN(rnn_cells, input_shape = (1, window_size)))\n",
    "model_rnn_error.add(Dense(1))\n",
    "model_rnn_error.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preguntas:\n",
    "- Que diferencia hay con la anterior? Solo hay un timestep.\n",
    "- Por que la diferencia en cantidad de parámetros?\n",
    "- Se puede pensar como un FNN (MLP)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFN (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fnn = Sequential()\n",
    "model_fnn.add(Dense(rnn_cells, input_shape=(window_size,) ))\n",
    "model_fnn.add(Dense(1))\n",
    "model_fnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RNN')\n",
    "print(model_rnn.layers[0].get_weights()[0].shape)\n",
    "print(model_rnn.layers[0].get_weights()[1].shape)\n",
    "print(model_rnn.layers[0].get_weights()[2].shape)\n",
    "print()\n",
    "print('RNN common error')\n",
    "print(model_rnn_error.layers[0].get_weights()[0].shape)\n",
    "print(model_rnn_error.layers[0].get_weights()[1].shape)\n",
    "print(model_rnn_error.layers[0].get_weights()[2].shape)\n",
    "print()\n",
    "print('MLP')\n",
    "print(model_fnn.layers[0].get_weights()[0].shape)\n",
    "print(model_fnn.layers[0].get_weights()[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Como se podría entrener los ejemplos de STOCK Market y texto de manera mas eficiente con Many-to-Many?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking RNNs\n",
    "### Podemos stackear varias capas?\n",
    "![stack%20units.png](stack units.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No funciona por que la salida no tiene la dimensión correcta\n",
    "# (67461, 100, 71)\n",
    "rnn_neurons = 100 # Hyperparametro para jugar\n",
    "time_steps = 100 # T\n",
    "n_features =  71# D\n",
    "\n",
    "input_shape = (time_steps, n_features)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(rnn_neurons, input_shape = input_shape))\n",
    "model.add(SimpleRNN(rnn_neurons))\n",
    "model.add(Dense(n_features, input_shape = input_shape, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por que falla?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (67461, 100, 71)\n",
    "rnn_neurons_1 = 200 # Hyperparametro para jugar\n",
    "rnn_neurons_2 = 150 # Hyperparametro para jugar\n",
    "time_steps = 100 # T\n",
    "n_features =  71# D\n",
    "input_shape = (time_steps, n_features)\n",
    "\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(rnn_neurons_1, input_shape = input_shape, return_sequences=True))\n",
    "model.add(SimpleRNN(rnn_neurons_2))\n",
    "model.add(Dense(n_features, input_shape = input_shape, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPTT (Back-Propagation Through Time)\n",
    "https://machinelearningmastery.com/truncated-backpropagation-through-time-in-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BPTT.png](BPTT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto podemos calcular la \"cross-entropy\" y derivar respecto a $W_o$,$W_h$ y W_x para hallar el gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BPTT_graph.png](BPTT_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problema: Vanishing/Exploding Gradient\n",
    "Soluciones:\n",
    "- Gradient Cliping\n",
    "- Truncated BPTT (Limitar la cantidad de unfoldings)\n",
    "- LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumiendo:\n",
    "## RNN vs Vanilla Network\n",
    "- Menor cantidad de parametros para resolver lo mismo\n",
    "- En principio una FFN podria resolver con la misma precisión un problema resuelto por una RNN, pero es demasiado complejo encontrar la solución\n",
    "- La estructura de la RNN simplifica la tarea de encontrar una solución de manera eficiente\n",
    "- Ya sabemos que el problema tiene naturaleza recurrente (necesitamos memoria)\n",
    "- FNN tienen que tener tamaño fijo en contraste con las RNNs\n",
    "\n",
    "## Las RNN pueden resolver problemas muy diferentes desde el punto de vista del tipo de procesamiento sobre la secuencia\n",
    "- Secuencia temporal modelada con \"longitud infinita\": Valor de acciones - Modelos tipo AR(N)\n",
    "- Secuencias finitas \"independientes\" unas de otras: Sentiment analisys (Longitud variable), paridad (Longitud fija)\n",
    "- Modelos de lenguaje: Por caracter o por palabra\n",
    "- Seq2Seq: Traducción, chatbots, Image captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM (Long Short Term Memory) y GRU (Gated Recurrent Unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos que un modelo de lenguaje genere texto de este tipo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Albert Einstein was a German-born theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics (alongside quantum mechanics). **His** work is also known for its influence on the philosophy of science. **He** is best known by the general public for his mass–energy equivalence formula E = mc2 (which has been dubbed \"the world's most famous equation\"). **He** received the 1921 Nobel Prize in Physics \"for **his** services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\", a pivotal step in the evolution of quantum theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitamos memoria a largo plazo, que \"recuerde\" que se esta hablando de una persona de genero masculino y respete la gramática a lo largo del texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lstm.png](lstm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- f -> forget Gate: \"Porcentaje\" que se queda de la LTM anterior\n",
    "- i -> Remeber Gate: \"Porcentaje\" que pasa de lo actual\n",
    "- o -> Learn Gate: \"Porcentaje\" a la salida o H (Recordar que h y la salida son lo mismo o STM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo Cantidad de parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "rnn_neurons = 1\n",
    "time_steps = 100 # T\n",
    "n_features = 1 # D\n",
    "input_shape = (time_steps, n_features)\n",
    "\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(rnn_neurons, input_shape = input_shape))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_number_of_LSTM_params = lambda rnn_neurons, n_features: (rnn_neurons*n_features + rnn_neurons**2 + rnn_neurons)*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_number_of_LSTM_params(rnn_neurons, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En las GRU $i_t$ y $f_t$ son complementarias $i_t + f_t=1$ por lo tanto las GRUs poseen menos parametros. Ademas, no se hace diferencia entre $c_t$ y $h_t$\n",
    "\n",
    "### En las LSTM $c_t$ y $h_t$ son diferentes por lo que el estado de la celula es diferente a la salida. Esto es diferente tanto en la simpleRNN como en la GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo usando Model Api de Keras\n",
    "https://keras.io/models/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, TimeDistributed\n",
    "\n",
    "features = 100\n",
    "cells = 256\n",
    "return_sequences=True\n",
    "\n",
    "input_data = Input(shape=(None, features), name=\"inputs\")\n",
    "LSTM_cell = LSTM(cells, return_state=True, return_sequences=return_sequences, name=\"Encoder_LSTM\")\n",
    "LSTM_outputs, LSTM_state_h, LSTM_state_c = LSTM_cell(input_data)\n",
    "\n",
    "print('LSTM')\n",
    "print(LSTM_outputs) # LSTM_outputs y LSTM_state_h son diferentes solo si return_sequences=True\n",
    "print(LSTM_state_h)\n",
    "print(LSTM_state_c)\n",
    "print()\n",
    "\n",
    "GRU_cell = GRU(cells, return_state=True, return_sequences=return_sequences, name=\"Encoder_GRU\")\n",
    "GRU_outputs, GRU_state_h = GRU_cell(input_data)\n",
    "print('GRU')\n",
    "print(GRU_outputs)\n",
    "print(GRU_state_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto es solo un ejemplo de como definir el modelo Model Api en Keras\n",
    "# La cantidad de neuronas de la GRU tiene que ser igual a la cantidad de features \n",
    "# con lo que sale de la LSTM. Tambien return_sequences tiene que ser True\n",
    "print(LSTM_outputs)\n",
    "\n",
    "stacked_output, _ = GRU_cell(LSTM_outputs)\n",
    "\n",
    "model = Model(inputs=input_data, outputs=stacked_output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Bidirectional\n",
    "features = 256\n",
    "cells = 256\n",
    "return_sequences=True\n",
    "\n",
    "input_data = Input(shape=(None, features))\n",
    "LSTM_cell = LSTM(cells, return_state=True, return_sequences=return_sequences)\n",
    "Bi_LSTM_cell = Bidirectional(LSTM_cell)\n",
    "print(Bi_LSTM_cell)\n",
    "LSTM_cell(input_data)\n",
    "#LSTM_outputs, LSTM_state_h2, LSTM_state_h1, LSTM_state_c1, LSTM_state_c2 = Bi_LSTM_cell(input_data)\n",
    "#print('LSTM')\n",
    "#print(LSTM_outputs) # LSTM_outputs y LSTM_state_h son diferentes solo si return_sequences=True\n",
    "#print(LSTM_state_h)\n",
    "#print(LSTM_state_c)\n",
    "#print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicaciones\n",
    "- Traducción\n",
    "- **No es buena** para predecir el futuro en secuencias temporales\n",
    "- Clasificación de imagenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BiLSTM-Image-classif.png](biLSTM-image-class-complete.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[](BiLSTM-Image-classif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
