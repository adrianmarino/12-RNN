{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"/Users/julianganzabal/facultad/lab-ml/mllab-tools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RNN_utils import get_deep_rnn, chars_to_one_hot, sample\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape =  (100, 69)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100, 200)          216000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 69)                13869     \n",
      "=================================================================\n",
      "Total params: 550,669\n",
      "Trainable params: 550,669\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=get_deep_rnn((100, 69), dense_units=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = model.get_layer('lstm_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 100, 69)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1.get_config()['batch_input_shape']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargo pesos entrenados con 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bodes_sangre_LSTM_deep\n",
    "# bodes_sangre_LSTM_deep_dropout_04\n",
    "model.load_weights('bodes_sangre_LSTM_deep_dropout_04.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data_and_get_dicts(text, train_fraction=0.6):\n",
    "    number_of_chars = len(text)\n",
    "    validation_index = int(number_of_chars*train_fraction)\n",
    "    text_train = text[:validation_index]\n",
    "    text_validation = text[validation_index:]\n",
    "    chars_train = set(text_train)\n",
    "    chars_test = set(text_validation)\n",
    "    chars_set = chars_train.intersection(chars_test)\n",
    "    chars = sorted(list(chars_set))\n",
    "    chars_to_indices = dict((c, i) for i, c in enumerate(chars))  # map each unique character to unique integer\n",
    "    indices_to_chars = dict((i, c) for i, c in enumerate(chars))  # map each unique integer back to unique character\n",
    "    return text_train, text_validation, chars_to_indices, indices_to_chars, chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = open('bodas_de_sangre.txt').read()\n",
    "text_train, text_validation, chars_to_indices, indices_to_chars, chars = split_data_and_get_dicts(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('training_chars', chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtengo frase para arrancar la predicción (semilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ovia. (Se va.)\n",
      "\n",
      "MOZO 1: (Entrando) ¡Tienes que beber con nosotros!\n",
      "\n",
      "NOVIO: Estoy esperando a la novi\n"
     ]
    }
   ],
   "source": [
    "window_size = 100\n",
    "initial_char = 0\n",
    "initial_text = text_validation[initial_char:window_size+initial_char]\n",
    "print(initial_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 69)\n"
     ]
    }
   ],
   "source": [
    "X_test = chars_to_one_hot(initial_text, chars, chars_to_indices, window_size)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                      estaba el novi 100\n"
     ]
    }
   ],
   "source": [
    "initial_text = ' '*86 + \"estaba la novi\"\n",
    "initial_text = ' '*86 + \"estaba el novi\"\n",
    "print(initial_text, len(initial_text))\n",
    "X_test = chars_to_one_hot(initial_text, chars, chars_to_indices, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hago argmax de la salida de la softmax - Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.69815001e-04   1.04711915e-04   6.12067524e-04   2.48154515e-06\n",
      "    2.64375831e-06   1.18511252e-05   3.08914750e-05   9.39381039e-07\n",
      "    6.34701678e-07   1.07057012e-07   1.09089467e-06   2.23332236e-07\n",
      "    3.93151364e-04   1.48102277e-04   4.25479413e-07   2.24685609e-05\n",
      "    3.36195371e-05   6.94645350e-05   2.46734942e-07   3.87413157e-10\n",
      "    5.02696730e-06   5.92376018e-05   2.55466057e-05   3.76178355e-06\n",
      "    2.82483252e-05   1.49229265e-06   5.11985490e-05   8.84169310e-07\n",
      "    1.03270338e-06   2.59097965e-06   2.63657262e-06   2.02024367e-07\n",
      "    2.75875063e-05   3.67782195e-06   2.11782748e-07   1.94094383e-07\n",
      "    3.08714896e-01   1.16438532e-04   9.80500947e-04   9.88472952e-04\n",
      "    4.05820981e-02   1.53034034e-05   1.13047042e-03   6.37602061e-04\n",
      "    7.40092807e-03   4.77217909e-05   9.11967643e-03   9.01302847e-04\n",
      "    3.71850037e-04   6.16845489e-01   5.09296806e-05   3.06225593e-05\n",
      "    6.11813681e-04   6.01137115e-04   2.02174461e-03   4.03822865e-04\n",
      "    5.81522356e-04   2.95946757e-05   1.55445596e-05   5.12018159e-05\n",
      "    3.88454673e-08   1.59354272e-08   4.32296976e-10   2.29762192e-03\n",
      "    5.26985037e-04   2.79337892e-05   8.18510234e-05   2.98587512e-03\n",
      "    1.06626621e-05]]\n",
      "49\n",
      "o\n"
     ]
    }
   ],
   "source": [
    "probs = model.predict(X_test)\n",
    "print(probs)\n",
    "index = np.argmax(probs)\n",
    "print(index)\n",
    "print(indices_to_chars[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordenados de mayor probabilidad a menor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['o', 'a', 'e', 'l', 'i', 'ó', 'á', 't', 'g', 'd']\n"
     ]
    }
   ],
   "source": [
    "print([indices_to_chars[i] for i in np.argsort(probs)[0][::-1]][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.616, 0.308, 0.04, 0.009, 0.007, 0.002, 0.002, 0.002, 0.001, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print([int(p*1000)/1000 for p in np.sort(probs)[0][::-1]][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Muestrar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5   0.25  0.15  0.1 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperatura = 1\n",
    "sample([0.5, 0.25, 0.15, 0.10], temperatura, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplos con 100 epochs\n",
    "### Primer muestreo, luego del espacio despues de la palabra 'ya' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOZO 1: (Entrando) ¡Tienes que beber con nosotros!\n",
      "\n",
      "NOVIO: Estoy esperando a la novia.\n",
      "\n",
      "MOZO 2: ¡Ya \n",
      "1.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities do not sum to 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-2f87964f36ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmy_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_to_chars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmy_sample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/facultad/lab-ml/mllab-tools/RNN_utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(a, temperature, verbose)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0msample_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_temp\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_temp\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mchoices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0;31m#return np.argmax(np.random.multinomial(1, sample_temp, 1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: probabilities do not sum to 1"
     ]
    }
   ],
   "source": [
    "window_size = 100\n",
    "initial_char = 16\n",
    "initial_text = text_validation[initial_char:window_size+initial_char]\n",
    "print(initial_text)\n",
    "initial_text = ' '*86 + \"estaba el novi\"\n",
    "X_test = chars_to_one_hot(initial_text, chars, chars_to_indices, window_size)\n",
    "probs = model.predict(X_test)\n",
    "print(probs[0].sum())\n",
    "for i in range(10):\n",
    "    my_sample = sample(probs[0], 1)\n",
    "    print(indices_to_chars[my_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MOZO 1: (Entrando) ¡Tienes que beber con nosotros!\\n\\nNOVIO: Estoy esperando a la novia.\\n\\nMOZO 2: ¡Ya '"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_text = text_validation[initial_char:window_size+initial_char]\n",
    "initial_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Luego de las letras 'novi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                      estaba la novi\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "window_size = 100\n",
    "initial_char = 0\n",
    "initial_text = text_validation[initial_char:window_size+initial_char]\n",
    "initial_text = ' '*86 + \"estaba la novi\"\n",
    "print(initial_text)\n",
    "X_test = chars_to_one_hot(initial_text, chars, chars_to_indices, window_size)\n",
    "probs = model.predict(X_test)\n",
    "for i in range(10):\n",
    "    my_sample = sample(probs[0], 0.5)\n",
    "    print(indices_to_chars[my_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion de generación de texto\n",
    "def generate_text(model, initial_text, N = 1000, window_size = 100, temperatura=0.5):\n",
    "    X_text_str = initial_text\n",
    "    print(initial_text)\n",
    "    print('--------------------------------------------------------------------------------')\n",
    "    print()\n",
    "    print(X_text_str, end='')\n",
    "    for i in range(N):\n",
    "        X_test = chars_to_one_hot(X_text_str[i:], chars, chars_to_indices, window_size)\n",
    "        probs = model.predict(X_test)\n",
    "        my_sample=sample(probs[0], temperatura)\n",
    "        new_char = indices_to_chars[my_sample]\n",
    "        X_text_str = X_text_str + new_char\n",
    "        print(new_char, end='')\n",
    "    return X_text_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poco entrenamiento: 1 Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                      estaba el novi\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "                                                                                      estaba el novina es e uo rmr de he n ale o a vora a clo . e eue uee e u e he a ena cemoe as  ea es ho ra ar e ao a"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'                                                                                      estaba el novina es e uo rmr de he n ale o a vora a clo . e eue uee e u e he a ena cemoe as  ea es ho ra ar e ao a'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('bodes_sangre_LSTM_deep_dropout_04_2_1_epochs.hdf5')\n",
    "initial_char = 16\n",
    "window_size = 100\n",
    "initial_text = text_validation[initial_char:window_size+initial_char]\n",
    "initial_text = ' '*86 + \"estaba el novi\"\n",
    "model.load_weights('bodes_sangre_LSTM_deep_dropout_04_2_1_epochs.hdf5')\n",
    "generate_text(model, initial_text, N = 100, window_size = window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrida 1 (100 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('bodes_sangre_LSTM_deep_dropout_04.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                      estaba el novi\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "                                                                                      estaba el novio?\n",
      "\n",
      "NOVIA: (Meriendo el azahar.) ¡Se puede la cadre, con el casido en la cuda!\n",
      "\n",
      "NOVIA: (Serie) Te me haber.\n",
      "\n",
      "NOVIA: Yo hay que me quiero esta cono salida de antanto y ahora mujer. ¿Y está y que está mejar?\n",
      "\n",
      "CRIADA: Eso de pueda mucho y no puede toda mucho y espera es de su para la mujer.\n",
      "\n",
      "LEONARDO: ¡Vamos.\n",
      "\n",
      "MADRE: Si para como usted de la caleda todo eso de novio. (Se pare a la mujer de la caballo de la cuerta de la mana con la boda.) ¡Vamos a la manada de azahar!\n",
      "\n",
      "LEONARDO: Pero de mueve en la carre. (Se siente la caballo en la carrello.)\n",
      "\n",
      "MADRE: (Seria)¡Ahara de casaría como un samiro, trando por el camino de la puerte de la cabeza. Pero me cuesto en la caballo y pode está con la mujer.\n",
      "\n",
      "MADRE: ¿Qué la visa! ¿Qué está que me fueda y siento está la cara a mí no quiere de agua es una cora una de casa a la corona de tu padre de la novia. Está puede está con la madre. (Se levana con la boda de la corana de azuello.) ¡Ay!\n",
      "\n",
      "CRIADA: (Cogiéndo)\n",
      "Duérmete, rosallo de la boda\n",
      "y el agua el rombr"
     ]
    }
   ],
   "source": [
    "output_text = generate_text(model, initial_text, N = 1000, window_size = window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrida 2 (A pesar de ser la misma entrada, distinto resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                      estaba el novi\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "                                                                                      estaba el novio!\n",
      "\n",
      "NOVIA: ¡¡¿y este seras de tienes con las regus!\n",
      "\n",
      "MADRE: (Levantándose) Yo se espuerte con castar de mi madre. (Le novia se la cabeza de azahar.)\n",
      "\n",
      "MADRE: (Entrando) No hay a me diente.\n",
      "\n",
      "NOVIO: (Serie) La media de su manido.\n",
      "\n",
      "LEONARDO: ¿Cuándo te para?\n",
      "\n",
      "NOVIA: A la viña es que la pera está me mano.\n",
      "\n",
      "LEONARDO: (Alegre) ¡Qué te aquí?\n",
      "\n",
      "MADRE: ¿Tú?\n",
      "\n",
      "NOVIO: Esa medido es estár que prena mucho y suego en tu hijo.\n",
      "\n",
      "NOVIO: ¿Y lo se puede a la casa?\n",
      "\n",
      "MADRE: Cuando como si lo que tengo una compra.\n",
      "\n",
      "NOVIO: Son mi mirido es una platan y pento de acarado.\n",
      "\n",
      "PADRE: ¡Cuando más que me puede comprar?\n",
      "\n",
      "NOVIA: ¿Cómo se?\n",
      "\n",
      "NOVIA: (Levantándole) ¡Qué arechas y bendejas de pierta se puede muchacha?\n",
      "\n",
      "MADRE: Sí que es una cosa. ¿Está que tengo y buenes que está que estás tú?\n",
      "\n",
      "LEONARDO: (Alegre) Pero ¡ú qué está con salí?\n",
      "\n",
      "CRIADA: Ahora está cosa.\n",
      "\n",
      "PADRE: ¡Aber casar, por el caballo que te casar que salí es un caballo toda se di mujer, y está con las campos y pala con el caballo de la corona. Para la conocia."
     ]
    }
   ],
   "source": [
    "output_text = generate_text(model, initial_text, N = 1000, window_size = window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                        \n",
      "\n",
      "MUCHACHAS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "                                                                                        \n",
      "\n",
      "MUCHACHAS: (((Levantándole) ¡Qué está que está que está que está que está que está que me puede que me puede q"
     ]
    }
   ],
   "source": [
    "initial_text = ' '*88+'\\n\\nMUCHACHAS:'\n",
    "len(initial_text)\n",
    "output_text = generate_text(model, initial_text, N = 100, window_size = window_size, \n",
    "                            temperatura = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                        \n",
      "\n",
      "MUCHACHAS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "                                                                                        \n",
      "\n",
      "MUCHACHAS: (((Levantándole) ¡Qué está que está que está que está que está que me puede que está que me puede q"
     ]
    }
   ],
   "source": [
    "initial_text = ' '*88+'\\n\\nMUCHACHAS:'\n",
    "len(initial_text)\n",
    "output_text = generate_text(model, initial_text, N = 100, window_size = window_size, \n",
    "                            temperatura = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mantiene estructuta de obra de teatro\n",
    "- Abre y cierra signo de pregunta y/o admiración\n",
    "- Abre y cierra parentesis\n",
    "- Articulo mas sustantivo con género\n",
    "- Comparar primera y segunda corrida y verificar que son diferentes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
